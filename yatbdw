#!/usr/bin/python
import urllib.request
import json
import bs4
import argparse
import distutils.dir_util
import os
import sys

cmdOptParser = argparse.ArgumentParser(description='This is a Tieba Pic Crawler')
cmdOptParser.add_argument('-t','--target',type=str,help='specify the target direcotry (default: creating a dir at pwd , named by thread title)')
cmdOptParser.add_argument('uri',type=str,help='the uri of the Tieba thread')
cmdOptParser.add_argument('-mp','--max-page',type=str,help='maximum pages to crawl')
cmdOptParser.add_argument('-fop','--follow-oringnal-post',nargs=0,help='maximum pages to crawl')
cmdOptParser.add_argument('-mf','--max-floor',type=str,help='maximum floors to crawl (Not implemented)')
cmdOptParser.add_argument('-l','--list-out',type=str,help='output the url list of pics')
cmdOptParser.add_argument('-p','--prefix',type=str,help='specify the prefix of pics (default:comic-)',default='comic-')

def crawlThroughtWholeThread(nEndPage,imgList,opid):
    for nP in list(range(2,int(nEndPage)+1)) :
        #list(range(2,2) will be null list , hence this won't be executed while there's only 1 page
        #print(threadUri+"?pn="+str(nP))
        req = urllib.request.urlopen(threadUri+"?pn="+str(nP))
        soup = bs4.BeautifulSoup(req)
        getSamePostersImage(soup.find_all("div",class_="l_post"),imgList,opid)

def getAllImgaeOfPost(imgList):
    imgList.extend([ image["src"] for image in soup.find_all("img",class_="BDE_Image")])

def getSamePostersImage(postList,imgList,opid):
    for postCur in postList :
        if opid == json.loads(postCur['data-field'])['author']['user_id'] :
            imgList.extend( [image['src'] for image in postCur.find_all("img",class_="BDE_Image")] )

def doImageDownload(imgSrcList,imgNamePrefix,targetDir):
    distutils.dir_util.mkpath(targetDir)
    picDigits = len(str(len(imgSrcList)))
    for idx , imgUri in enumerate(imgSrcList):
        fileNameWithPath = str(targetDir)+"/"+str(imgNamePrefix)+str(idx).zfill(picDigits)+".jpg"
        print("Downloading #"+str(idx)+" :"+fileNameWithPath)
        imageData = urllib.request.urlopen(imgUri).read()
        imageFile = open(fileNameWithPath,'wb')
        imageFile.write(imageData)
        imageFile.close()

def doListOutput(imgSrcList,logFileName):
    logFile = open(str(logFileName),'w')
    for idx , imgUri in enumerate(imgSrcList):
        print(str(idx)+' '+str(imgUri))
        logFile.write(str(idx)+' '+str(imgUri)+'\n')
    logFile.close()
    print("""You may try:" cat """+str(logFileName)+""" | while read n f; do wget "$f" -O "comic-$n.jpg"; done" for downloading .""")

def getThreadInfo(threadUri,customMaxPage,followOP):
    req = urllib.request.urlopen(threadUri)
    soup = bs4.BeautifulSoup(req)
    maxFloor = "".join(soup.find_all("span",class_="red")[0].contents)
    #TODO : implement the download range defined by given num of max floors.
    maxPage = "".join(soup.find_all("span",class_="red")[1].contents)
    #TODO : implement the download range defined by given num of max page num.
    tTile = "".join([ tCur["title"] for tCur in soup.find_all("h1",class_="core_title_txt")])

    #in order to get only the OP's post , we need to get the user_id
    uid = json.loads(soup.find("div","l_post")['data-field'])['author']['user_id']

    imgSrcList = []

    getSamePostersImage(soup.find_all("div",class_="l_post"),imgSrcList,uid)

    print("Only download till pg. #"+str(customMaxPage)+" .")
    if customMaxPage != None :
        if customMaxPage <= maxPage :
            crawlThroughtWholeThread(customMaxPage,imgSrcList)
        else :
            print("The maximum page you request is out of bound , which is : "+str(maxPage))
            exit()
    else :
        crawlThroughtWholeThread(maxPage,imgSrcList,uid)
    
    print("This is a debug message :\n",imgSrcList)

    return tTile , imgSrcList


if __name__ == '__main__' :
    cmdArgs = cmdOptParser.parse_args()
    threadTitle , imgList = getThreadInfo(cmdArgs.uri,cmdArgs.max_page,cmdArgs.follow_oringnal_post)
    if cmdArgs.list_out == None :
        if cmdArgs.target == None :
            doImageDownload(imgList,cmdArgs.prefix,threadTitle)
        else :
            doImageDownload(imgList,cmdArgs.prefix,cmdArgs.target)
    else :
        doListOutput(imgList,cmdArgs.list_out) 
